import streamlit as st
import pandas as pd
import feedparser
import urllib.parse
from datetime import datetime
from duckduckgo_search import DDGS

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="å…¨çƒå»šé›»æƒ…å ±ç­–å±•å°", page_icon="ğŸ¯", layout="wide")
st.title("ğŸ¯ å…¨çƒå»šé›»æƒ…å ±ç­–å±•å° (Search & Select)")

# --- 2. åˆå§‹åŒ– Session State (ç”¨ä¾†æš«å­˜ä½ çš„ç²¾é¸æ¸…å–®) ---
if 'favorites' not in st.session_state:
    st.session_state.favorites = pd.DataFrame(columns=['Date', 'Title', 'Link', 'Source'])

if 'search_results' not in st.session_state:
    st.session_state.search_results = pd.DataFrame()

# --- 3. æ ¸å¿ƒçˆ¬èŸ²å‡½æ•¸ ---

def fetch_google_news(keyword, lang, region):
    # æ™ºæ…§æœå°‹é‚è¼¯
    search_query = keyword
    target_gl = region
    target_ceid = f"{region}:{lang.split('-')[0]}"
    
    if (region in ["US", "CA"]) and ("zh" in lang):
        if region == "US": search_query = f"{keyword} (ç¾åœ‹ OR åŒ—ç¾ OR USA)"
        elif region == "CA": search_query = f"{keyword} (åŠ æ‹¿å¤§ OR Canada OR æ¸©å“¥å OR å¤šä¼¦å¤š)"
        target_gl = "TW"
        target_ceid = "TW:zh-Hant"

    if region == "HK" and "zh" in lang:
        target_ceid = "HK:zh-Hant"
        target_gl = "HK"

    encoded_keyword = urllib.parse.quote(search_query)
    target_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl={lang}&gl={target_gl}&ceid={target_ceid}"
    
    try:
        feed = feedparser.parse(target_url)
        data = []
        for entry in feed.entries:
            try:
                pub_date = datetime(*entry.published_parsed[:6])
            except:
                pub_date = datetime.now()
            data.append({
                "Select": False, # é è¨­ä¸å‹¾é¸
                "Date": pub_date,
                "Type": "æ–°è",
                "Title": entry.title,
                "Source": entry.source.title if 'source' in entry else "Google News",
                "Link": entry.link
            })
        return pd.DataFrame(data)
    except:
        return pd.DataFrame()

def fetch_web_search(keyword, region_code, time_range):
    if region_code == "US": ddg_region = "us-en"
    elif region_code == "CA": ddg_region = "ca-en"
    elif region_code == "HK": ddg_region = "hk-tzh"
    else: ddg_region = "wt-wt"
    
    ddg_time = None 
    if time_range == "éå»ä¸€å¤©": ddg_time = "d"
    elif time_range == "éå»ä¸€é€±": ddg_time = "w"
    elif time_range == "éå»ä¸€å€‹æœˆ": ddg_time = "m"
    elif time_range == "éå»ä¸€å¹´": ddg_time = "y"
    
    final_keyword = keyword
    search_region = ddg_region
    is_chinese_query = any(u'\u4e00' <= c <= u'\u9fff' for c in keyword)
    
    if (region_code in ["US", "CA"]) and is_chinese_query:
        search_region = "wt-wt"
        if region_code == "US": final_keyword = f"{keyword} (ç¾åœ‹ OR åŒ—ç¾ OR è¯äºº)"
        elif region_code == "CA": final_keyword = f"{keyword} (åŠ æ‹¿å¤§ OR æ¸©å“¥è¯ OR å¤šå€«å¤š)"

    try:
        results = DDGS().text(keywords=final_keyword, region=search_region, time=ddg_time, max_results=40)
        data = []
        for r in results:
            data.append({
                "Select": False,
                "Date": datetime.now(),
                "Type": "å…¨ç¶²",
                "Title": r['title'],
                "Source": urllib.parse.urlparse(r['href']).netloc,
                "Link": r['href']
            })
        return pd.DataFrame(data)
    except:
        return pd.DataFrame()

def run_hybrid_search(keyword, location_choice, search_types, time_range):
    frames = []
    if location_choice == "ğŸ‡ºğŸ‡¸ ç¾åœ‹ (US)":
        news_tasks = [("en-US", "US"), ("zh-TW", "US")]
        region_code = "US"
    elif location_choice == "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ (CA)":
        news_tasks = [("en-CA", "CA"), ("zh-TW", "CA")]
        region_code = "CA"
    elif location_choice == "ğŸ‡­ğŸ‡° é¦™æ¸¯ (HK)":
        news_tasks = [("zh-HK", "HK"), ("en-HK", "HK")]
        region_code = "HK"
    
    if "æ–°èåª’é«” (News)" in search_types:
        if time_range in ["ä¸é™æ™‚é–“ (é è¨­)", "éå»ä¸€å¤©", "éå»ä¸€é€±", "éå»ä¸€å€‹æœˆ"]:
            for lang, region in news_tasks:
                frames.append(fetch_google_news(keyword, lang, region))
            
    if "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Forums/Blogs)" in search_types:
        frames.append(fetch_web_search(keyword, region_code, time_range))

    if frames:
        result = pd.concat(frames).drop_duplicates(subset=['Link'])
        # ç¢ºä¿ Select æ¬„ä½åœ¨æœ€å‰é¢
        cols = ['Select'] + [c for c in result.columns if c != 'Select']
        return result[cols]
    else:
        return pd.DataFrame()

# --- 4. å´é‚Šæ¬„ (é¡¯ç¤ºç²¾é¸æ¸…å–®) ---
with st.sidebar:
    st.header("ğŸŒŸ æˆ‘çš„ç²¾é¸æ¸…å–®")
    
    if not st.session_state.favorites.empty:
        fav_count = len(st.session_state.favorites)
        st.success(f"ç›®å‰å·²æ”¶è— {fav_count} ç­†è³‡æ–™")
        
        # é¡¯ç¤ºæ¸…å–®é è¦½
        st.dataframe(
            st.session_state.favorites[['Title', 'Source']], 
            use_container_width=True, 
            hide_index=True
        )
        
        # ä¸‹è¼‰æŒ‰éˆ•
        csv = st.session_state.favorites.to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ç²¾é¸æ¸…å–® (CSV)",
            data=csv,
            file_name=f'kitchen_favorites_{datetime.now().strftime("%Y%m%d")}.csv',
            mime='text/csv',
        )
        
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ¸…å–®"):
            st.session_state.favorites = pd.DataFrame(columns=['Date', 'Title', 'Link', 'Source'])
            st.rerun()
    else:
        st.info("å°šæœªæ”¶è—ä»»ä½•è³‡æ–™ã€‚è«‹åœ¨å³å´æœå°‹å¾Œå‹¾é¸åŠ å…¥ã€‚")

# --- 5. ä¸»ç•«é¢ (æœå°‹èˆ‡æŒ‘é¸) ---

st.subheader("ğŸ“¡ æƒ…å ±æœå°‹")
col1, col2, col3 = st.columns([2, 1, 1])
with col1:
    search_kw = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚: æŠ½æ²¹ç…™æ©Ÿ, æ–¹å¤ª, Robam...")
with col2:
    location = st.selectbox("ç›®æ¨™å¸‚å ´", ["ğŸ‡ºğŸ‡¸ ç¾åœ‹ (US)", "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ (CA)", "ğŸ‡­ğŸ‡° é¦™æ¸¯ (HK)"])
with col3:
    time_range = st.selectbox("â±ï¸ æ™‚é–“ç¯„åœ", ["ä¸é™æ™‚é–“ (é è¨­)", "éå»ä¸€å¤©", "éå»ä¸€é€±", "éå»ä¸€å€‹æœˆ", "éå»ä¸€å¹´"])

search_scope = st.multiselect(
    "æœå°‹ä¾†æº",
    ["æ–°èåª’é«” (News)", "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Forums/Blogs)"],
    default=["æ–°èåª’é«” (News)", "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Forums/Blogs)"]
)

# æœå°‹æŒ‰éˆ•
if st.button("ğŸš€ æœå°‹", type="primary"):
    if search_kw:
        with st.spinner("æ­£åœ¨æœå°‹ä¸­..."):
            st.session_state.search_results = run_hybrid_search(search_kw, location, search_scope, time_range)

# é¡¯ç¤ºæœå°‹çµæœ (å¦‚æœæœ‰çš„è©±)
if not st.session_state.search_results.empty:
    st.divider()
    st.markdown(f"### ğŸ” æœå°‹çµæœ (å…± {len(st.session_state.search_results)} ç­†)")
    st.info("è«‹å‹¾é¸ä½ è¦ºå¾—æœ‰åƒ¹å€¼çš„æƒ…å ±ï¼Œç„¶å¾Œé»æ“Šä¸‹æ–¹çš„ã€ŒåŠ å…¥ç²¾é¸ã€æŒ‰éˆ•ã€‚")
    
    # ä½¿ç”¨ Data Editor è®“ä½¿ç”¨è€…å¯ä»¥å‹¾é¸
    edited_df = st.data_editor(
        st.session_state.search_results,
        column_config={
            "Select": st.column_config.CheckboxColumn("æ”¶è—", help="å‹¾é¸ä»¥åŠ å…¥æ¸…å–®", width="small"),
            "Link": st.column_config.LinkColumn("é€£çµ", display_text="Go", width="small"),
            "Date": st.column_config.DateColumn("æ—¥æœŸ", format="YYYY-MM-DD", width="small"),
            "Title": st.column_config.TextColumn("æ¨™é¡Œ"),
        },
        use_container_width=True,
        hide_index=True,
        key="data_editor" # é—œéµï¼šçµ¦å®š key æ‰èƒ½æŠ“åˆ°ç‹€æ…‹
    )
    
    # åŠ å…¥ç²¾é¸æŒ‰éˆ•
    if st.button("ğŸŒŸ å°‡å‹¾é¸é …ç›®åŠ å…¥ç²¾é¸æ¸…å–®"):
        # æ‰¾å‡ºè¢«å‹¾é¸çš„è¡Œ
        selected_rows = edited_df[edited_df['Select'] == True]
        
        if not selected_rows.empty:
            # ç§»é™¤ Select æ¬„ä½å¾ŒåŠ å…¥ç²¾é¸
            to_add = selected_rows.drop(columns=['Select'])
            st.session_state.favorites = pd.concat([st.session_state.favorites, to_add]).drop_duplicates(subset=['Link'])
            st.success(f"æˆåŠŸåŠ å…¥ {len(selected_rows)} ç­†è³‡æ–™ï¼è«‹çœ‹å·¦å´å´é‚Šæ¬„ã€‚")
            st.rerun() # é‡æ–°æ•´ç†é é¢ä»¥æ›´æ–°å´é‚Šæ¬„
        else:
            st.warning("ä½ é‚„æ²’å‹¾é¸ä»»ä½•é …ç›®å–”ï¼")

elif search_kw:
    st.warning("æŒ‰ä¸€ä¸‹æœå°‹æŒ‰éˆ•é–‹å§‹æ‰¾è³‡æ–™å§ï¼")
