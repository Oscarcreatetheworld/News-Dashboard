import streamlit as st
import pandas as pd
import feedparser
import urllib.parse
from datetime import datetime, timedelta
import altair as alt

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="åŒ—ç¾å»šé›»æƒ…è³‡ä¸­å¿ƒ", page_icon="ğŸ³", layout="wide")
st.title("ğŸ³ åŒ—ç¾å»šé›»æƒ…è³‡ä¸­å¿ƒ (Live & Database)")

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•¸ ---

# A. çˆ¬èŸ²å‡½æ•¸ (å³æ™‚æŠ“å–ç”¨)
def fetch_live_news(keyword, lang_code="en-US", region="US"):
    encoded_keyword = urllib.parse.quote(keyword)
    # åˆ¤æ–·èªè¨€ä»£ç¢¼è™•ç†
    ceid_lang = lang_code.split('-')[0]
    target_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl={lang_code}&gl={region}&ceid={region}:{ceid_lang}"
    
    feed = feedparser.parse(target_url)
    data = []
    for entry in feed.entries:
        try:
            pub_date = datetime(*entry.published_parsed[:6])
        except:
            pub_date = datetime.now()
            
        data.append({
            "Date": pub_date,
            "Title": entry.title,
            "Source": entry.source.title if 'source' in entry else "N/A",
            "Link": entry.link
        })
    return pd.DataFrame(data)

# B. è³‡æ–™åº«è®€å–å‡½æ•¸ (æ­·å²è³‡æ–™ç”¨)
@st.cache_data(ttl=600) # è¨­å®šå¿«å– 10 åˆ†é˜ï¼Œé¿å…ä¸€ç›´è®€å– Google Sheet
def load_historical_data(url):
    try:
        df = pd.read_csv(url)
        df['Date'] = pd.to_datetime(df['Date'])
        return df
    except:
        return pd.DataFrame()

# --- 3. å´é‚Šæ¬„è¨­å®š ---
with st.sidebar:
    st.header("âš™ï¸ ç³»çµ±è¨­å®š")
    mode = st.radio("é¸æ“‡æ¨¡å¼", ["ğŸ“¡ å³æ™‚åµå¯Ÿ (Live Search)", "ğŸ—„ï¸ æ­·å²è³‡æ–™åº« (Database)"])
    
    st.divider()
    
    # æ—¥æœŸç¯©é¸å™¨ (é è¨­åŠå¹´å…§)
    today = datetime.now().date()
    half_year_ago = today - timedelta(days=180)
    
    st.subheader("ğŸ“… æ—¥æœŸç¯©é¸")
    start_date = st.date_input("é–‹å§‹æ—¥æœŸ", half_year_ago)
    end_date = st.date_input("çµæŸæ—¥æœŸ", today)

# --- 4. ä¸»ç•«é¢é‚è¼¯ ---

# === æ¨¡å¼ä¸€ï¼šå³æ™‚åµå¯Ÿ (éš¨æ„æ›´æ”¹é—œéµå­—) ===
if mode == "ğŸ“¡ å³æ™‚åµå¯Ÿ (Live Search)":
    st.subheader("ğŸ“¡ å³æ™‚å…¨ç¶²æœç´¢")
    st.info("æ­¤æ¨¡å¼æœƒç›´æ¥é€£ç·š Google News æŠ“å–ç•¶ä¸‹æœ€æ–°è³‡è¨Š (é©åˆè‡¨æ™‚æŸ¥è©¢æ–°ç«¶å“)")
    
    col1, col2 = st.columns([3, 1])
    with col1:
        search_kw = st.text_input("è¼¸å…¥æƒ³æŸ¥è©¢çš„é—œéµå­— (æ”¯æ´ä¸­è‹±æ–‡)", "Air Fryer")
    with col2:
        region_opt = st.selectbox("åœ°å€/èªè¨€", ["ğŸ‡ºğŸ‡¸ ç¾åœ‹ (è‹±æ–‡)", "ğŸ‡ºğŸ‡¸ ç¾åœ‹ (ä¸­æ–‡)", "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ (è‹±æ–‡)"])
    
    # è¨­å®šåƒæ•¸
    lang_map = {
        "ğŸ‡ºğŸ‡¸ ç¾åœ‹ (è‹±æ–‡)": ("en-US", "US"),
        "ğŸ‡ºğŸ‡¸ ç¾åœ‹ (ä¸­æ–‡)": ("zh-TW", "US"),
        "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ (è‹±æ–‡)": ("en-CA", "CA")
    }
    
    if st.button("ğŸš€ é–‹å§‹æœç´¢", type="primary"):
        with st.spinner(f"æ­£åœ¨æƒæåŒ—ç¾ç¶²è·¯é—œæ–¼ '{search_kw}' çš„è³‡è¨Š..."):
            lang, reg = lang_map[region_opt]
            live_df = fetch_live_news(search_kw, lang, reg)
            
            if not live_df.empty:
                st.success(f"æœå°‹å®Œæˆï¼æ‰¾åˆ° {len(live_df)} ç­†æœ€æ–°è³‡æ–™")
                
                # é¡¯ç¤ºè³‡æ–™
                st.data_editor(
                    live_df,
                    column_config={
                        "Link": st.column_config.LinkColumn("é–±è®€é€£çµ"),
                        "Date": st.column_config.DateColumn("ç™¼å¸ƒæ™‚é–“", format="YYYY-MM-DD HH:mm"),
                    },
                    use_container_width=True
                )
            else:
                st.warning("æ‰¾ä¸åˆ°è¿‘æœŸç›¸é—œæ–°èï¼Œè«‹å˜—è©¦æ›´æ›é—œéµå­—ã€‚")

# === æ¨¡å¼äºŒï¼šæ­·å²è³‡æ–™åº« (æŸ¥çœ‹ç´¯ç©çš„è³‡æ–™) ===
elif mode == "ğŸ—„ï¸ æ­·å²è³‡æ–™åº« (Database)":
    st.subheader("ğŸ—„ï¸ å…§éƒ¨è¼¿æƒ…è³‡æ–™åº«")
    
    # ğŸ”¥ è«‹è¨˜å¾—æŠŠé€™è£¡æ›æˆä½ çš„ CSV ç¶²å€ ğŸ”¥
    sheet_url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vQai1zkVJlpDcZhzs76S_JiCsm1JogWxdYlw4vA4k1IeWLHqiReRRY29xQm7ephIk9QJfri7OlvfdmF/pub?output=csv"
    
    df = load_historical_data(sheet_url)
    
    if not df.empty:
        # æ—¥æœŸç¯©é¸é‚è¼¯
        mask = (df['Date'].dt.date >= start_date) & (df['Date'].dt.date <= end_date)
        filtered_df = df.loc[mask]
        
        # é—œéµå­—ç¯©é¸ (åœ¨æ­·å²è³‡æ–™ä¸­æœå°‹)
        db_search = st.text_input("åœ¨è³‡æ–™åº«ä¸­æœå°‹æ¨™é¡Œ...", placeholder="è¼¸å…¥é—œéµå­—ç¯©é¸æ­·å²è³‡æ–™")
        if db_search:
            filtered_df = filtered_df[filtered_df['Title'].str.contains(db_search, case=False, na=False)]
            
        # é¡¯ç¤ºçµ±è¨ˆæ•¸æ“š
        c1, c2, c3 = st.columns(3)
        c1.metric("é¸å®šæœŸé–“è³‡æ–™é‡", f"{len(filtered_df)} ç­†")
        c2.metric("è³‡æ–™èµ·å§‹æ—¥", f"{start_date}")
        c3.metric("è³‡æ–™çµæŸæ—¥", f"{end_date}")
        
        # ç¹ªè£½è²é‡åœ–
        if not filtered_df.empty:
            st.markdown("### ğŸ“Š æœŸé–“è²é‡è¶¨å‹¢")
            trend = filtered_df.groupby(filtered_df['Date'].dt.date).size().reset_index(name='Count')
            st.bar_chart(trend.set_index('Date'), color="#FF4B4B")

            st.markdown("### ğŸ“‹ è©³ç´°è³‡æ–™è¡¨")
            st.data_editor(
                filtered_df,
                column_config={
                    "Link": st.column_config.LinkColumn("é€£çµ"),
                    "Date": st.column_config.DateColumn("æ—¥æœŸ", format="YYYY-MM-DD"),
                },
                hide_index=True,
                use_container_width=True
            )
        else:
            st.warning("é¸å®šçš„æ™‚é–“ç¯„åœå…§æ²’æœ‰è³‡æ–™ã€‚")
            
    else:
        st.error("ç„¡æ³•é€£æ¥è³‡æ–™åº«ï¼Œè«‹æª¢æŸ¥ CSV ç¶²å€ã€‚")
