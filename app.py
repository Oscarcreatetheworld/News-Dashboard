import streamlit as st
import pandas as pd
import feedparser
import urllib.parse
from datetime import datetime, timedelta
from duckduckgo_search import DDGS

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="å…¨çƒå»šé›»å…¨ç¶²é›·é” Lite", page_icon="ğŸ“¡", layout="wide")
st.title("ğŸ“¡ å…¨çƒå»šé›»å…¨ç¶²é›·é” (Lite)")

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•¸ ---

# A. Google News çˆ¬èŸ² (ç„¡æ‘˜è¦ç‰ˆ)
def fetch_google_news(keyword, lang, region):
    # æ™ºæ…§æœå°‹é‚è¼¯
    search_query = keyword
    target_gl = region
    target_ceid = f"{region}:{lang.split('-')[0]}"
    
    # é‡å°åŒ—ç¾ä¸­æ–‡çš„ç‰¹æ®Šè™•ç† (é—œéµå­—æ¤å…¥ + æ”¾å¯¬åœ°å€)
    if (region in ["US", "CA"]) and ("zh" in lang):
        if region == "US":
            search_query = f"{keyword} (ç¾åœ‹ OR åŒ—ç¾ OR USA)"
        elif region == "CA":
            search_query = f"{keyword} (åŠ æ‹¿å¤§ OR Canada OR æ¸©å“¥å OR å¤šä¼¦å¤š)"
        target_gl = "TW" 
        target_ceid = "TW:zh-Hant"

    # é¦™æ¸¯ç¶­æŒåŸæ¨£
    if region == "HK" and "zh" in lang:
        target_ceid = "HK:zh-Hant"
        target_gl = "HK"

    encoded_keyword = urllib.parse.quote(search_query)
    target_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl={lang}&gl={target_gl}&ceid={target_ceid}"
    
    try:
        feed = feedparser.parse(target_url)
        data = []
        for entry in feed.entries:
            try:
                pub_date = datetime(*entry.published_parsed[:6])
            except:
                pub_date = datetime.now()
                
            data.append({
                "Date": pub_date,
                "Type": "æ–°è",
                "Title": entry.title,
                "Source": entry.source.title if 'source' in entry else "Google News",
                "Link": entry.link
            })
        return pd.DataFrame(data)
    except:
        return pd.DataFrame()

# B. DuckDuckGo å…¨ç¶²çˆ¬èŸ² (ç„¡æ‘˜è¦ç‰ˆ)
def fetch_web_search(keyword, region_code, time_range):
    if region_code == "US": ddg_region = "us-en"
    elif region_code == "CA": ddg_region = "ca-en"
    elif region_code == "HK": ddg_region = "hk-tzh"
    else: ddg_region = "wt-wt"
    
    ddg_time = None 
    if time_range == "éå»ä¸€å¤©": ddg_time = "d"
    elif time_range == "éå»ä¸€é€±": ddg_time = "w"
    elif time_range == "éå»ä¸€å€‹æœˆ": ddg_time = "m"
    elif time_range == "éå»ä¸€å¹´": ddg_time = "y"
    
    # ä¸­æ–‡æœå°‹å„ªåŒ–
    final_keyword = keyword
    search_region = ddg_region
    is_chinese_query = any(u'\u4e00' <= c <= u'\u9fff' for c in keyword)
    
    if (region_code in ["US", "CA"]) and is_chinese_query:
        search_region = "wt-wt"
        if region_code == "US":
            final_keyword = f"{keyword} (ç¾åœ‹ OR åŒ—ç¾ OR è¯äºº)"
        elif region_code == "CA":
            final_keyword = f"{keyword} (åŠ æ‹¿å¤§ OR æ¸©å“¥è¯ OR å¤šå€«å¤š)"

    try:
        results = DDGS().text(keywords=final_keyword, region=search_region, time=ddg_time, max_results=40)
        
        data = []
        for r in results:
            data.append({
                "Date": datetime.now(),
                "Type": "å…¨ç¶²",
                "Title": r['title'],
                "Source": urllib.parse.urlparse(r['href']).netloc,
                "Link": r['href']
            })
        return pd.DataFrame(data)
    except Exception as e:
        return pd.DataFrame()

# C. æ··åˆæœç´¢æ§åˆ¶å™¨
def run_hybrid_search(keyword, location_choice, search_types, time_range):
    frames = []
    
    if location_choice == "ğŸ‡ºğŸ‡¸ ç¾åœ‹ (US)":
        news_tasks = [("en-US", "US"), ("zh-TW", "US")]
        region_code = "US"
    elif location_choice == "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ (CA)":
        news_tasks = [("en-CA", "CA"), ("zh-TW", "CA")]
        region_code = "CA"
    elif location_choice == "ğŸ‡­ğŸ‡° é¦™æ¸¯ (HK)":
        news_tasks = [("zh-HK", "HK"), ("en-HK", "HK")]
        region_code = "HK"
    
    if "æ–°èåª’é«” (News)" in search_types:
        if time_range in ["ä¸é™æ™‚é–“ (é è¨­)", "éå»ä¸€å¤©", "éå»ä¸€é€±", "éå»ä¸€å€‹æœˆ"]:
            for lang, region in news_tasks:
                df = fetch_google_news(keyword, lang, region)
                frames.append(df)
            
    if "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Forums/Blogs)" in search_types:
        df_web = fetch_web_search(keyword, region_code, time_range)
        frames.append(df_web)

    if frames:
        result_df = pd.concat(frames)
        result_df = result_df.drop_duplicates(subset=['Link'])
        return result_df
    else:
        return pd.DataFrame()

# D. è³‡æ–™åº«è®€å–
@st.cache_data(ttl=600)
def load_historical_data(url):
    try:
        df = pd.read_csv(url)
        df['Date'] = pd.to_datetime(df['Date'])
        return df
    except:
        return pd.DataFrame()

# --- 3. å´é‚Šæ¬„ ---
with st.sidebar:
    st.header("âš™ï¸ é›·é”è¨­å®š")
    mode = st.radio("æ¨¡å¼", ["ğŸ“¡ å…¨ç¶²æƒæ (Live)", "ğŸ—„ï¸ æ­·å²è³‡æ–™åº«"])
    st.divider()
    today = datetime.now().date()
    start_date = st.date_input("è³‡æ–™åº«èµ·å§‹æ—¥", today - timedelta(days=180))

# --- 4. ä¸»ç•«é¢ ---

if mode == "ğŸ“¡ å…¨ç¶²æƒæ (Live)":
    st.subheader("ğŸ“¡ å…¨çƒå»šé›»å…¨ç¶²æƒæ")
    
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        search_kw = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚: æŠ½æ²¹ç…™æ©Ÿ, æ–¹å¤ª, Robam...")
    with col2:
        location = st.selectbox("ç›®æ¨™å¸‚å ´", ["ğŸ‡ºğŸ‡¸ ç¾åœ‹ (US)", "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ (CA)", "ğŸ‡­ğŸ‡° é¦™æ¸¯ (HK)"])
    with col3:
        time_range = st.selectbox("â±ï¸ æ™‚é–“ç¯„åœ", ["ä¸é™æ™‚é–“ (é è¨­)", "éå»ä¸€å¤©", "éå»ä¸€é€±", "éå»ä¸€å€‹æœˆ", "éå»ä¸€å¹´"])
        
    search_scope = st.multiselect(
        "é¸æ“‡æœå°‹ä¾†æº",
        ["æ–°èåª’é«” (News)", "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Forums/Blogs)"],
        default=["æ–°èåª’é«” (News)", "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Forums/Blogs)"]
    )
    
    if st.button("ğŸš€ ç™¼å°„é›·é”", type="primary"):
        if search_kw:
            with st.spinner(f"æ­£åœ¨æŒ–æ˜ {location} çš„ç›¸é—œæƒ…å ±..."):
                df = run_hybrid_search(search_kw, location, search_scope, time_range)
                
                if not df.empty:
                    st.success(f"æƒæå®Œæˆï¼å…±ç™¼ç¾ {len(df)} ç­†æƒ…å ±")
                    st.data_editor(
                        df,
                        column_config={
                            "Link": st.column_config.LinkColumn("é€£çµ", display_text="Go", width="small"),
                            "Date": st.column_config.DateColumn("æ—¥æœŸ", format="YYYY-MM-DD", width="small"),
                            "Type": st.column_config.TextColumn("é¡å‹", width="small"),
                            "Source": st.column_config.TextColumn("ä¾†æº", width="medium"),
                            "Title": st.column_config.TextColumn("æ¨™é¡Œ"), # è®“æ¨™é¡Œè‡ªå‹•å¡«æ»¿å‰©é¤˜ç©ºé–“
                        },
                        use_container_width=True
                    )
                else:
                    st.warning("æ‰¾ä¸åˆ°çµæœã€‚")
        else:
            st.error("è«‹è¼¸å…¥é—œéµå­—")

elif mode == "ğŸ—„ï¸ æ­·å²è³‡æ–™åº«":
    st.subheader("ğŸ—„ï¸ å…§éƒ¨è¼¿æƒ…è³‡æ–™åº«")
    sheet_url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vQai1zkVJlpDcZhzs76S_JiCsm1JogWxdYlw4vA4k1IeWLHqiReRRY29xQm7ephIk9QJfri7OlvfdmF/pubhtml"
    
    df = load_historical_data(sheet_url)
    if not df.empty:
        mask = (df['Date'].dt.date >= start_date)
        filtered_df = df.loc[mask]
        
        db_search = st.text_input("æœå°‹è³‡æ–™åº«...", placeholder="è¼¸å…¥é—œéµå­—...")
        if db_search:
            filtered_df = filtered_df[filtered_df['Title'].str.contains(db_search, case=False, na=False)]
            
        st.data_editor(filtered_df, use_container_width=True, hide_index=True)
    else:
        st.error("ç„¡æ³•è®€å–è³‡æ–™åº«")
