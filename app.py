import streamlit as st
import pandas as pd
import feedparser
import urllib.parse
from datetime import datetime
from duckduckgo_search import DDGS

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="å…¨çƒå»šé›»æƒ…å ±ä¸­å¿ƒ Pro", page_icon="ğŸ³", layout="wide")

# --- 2. Session State åˆå§‹åŒ– ---
if 'favorites' not in st.session_state:
    st.session_state.favorites = pd.DataFrame(columns=['Folder', 'Date', 'Title', 'Link', 'Source'])

if 'folder_list' not in st.session_state:
    st.session_state.folder_list = ["ğŸ“¥ æœªåˆ†é¡", "ğŸ”¥ æ–¹å¤ª (Fotile)", "ğŸ”¥ è€é—† (Robam)", "ğŸ‡ªğŸ‡º æ­ç³»å“ç‰Œ", "ğŸ‡ºğŸ‡¸ ç¾ç³»å“ç‰Œ"]

if 'search_results' not in st.session_state:
    st.session_state.search_results = pd.DataFrame()

# --- 3. çˆ¬èŸ²å‡½æ•¸ç¾¤ ---

# A. æ–°èçˆ¬èŸ² (Google News)
def fetch_google_news(keyword, lang, region):
    search_query = keyword
    target_gl = region
    target_ceid = f"{region}:{lang.split('-')[0]}"
    
    if (region in ["US", "CA"]) and ("zh" in lang):
        if region == "US": search_query = f"{keyword} (ç¾åœ‹ OR åŒ—ç¾ OR USA)"
        elif region == "CA": search_query = f"{keyword} (åŠ æ‹¿å¤§ OR Canada OR æ¸©å“¥å OR å¤šä¼¦å¤š)"
        target_gl = "TW"
        target_ceid = "TW:zh-Hant"

    if region == "HK" and "zh" in lang:
        target_ceid = "HK:zh-Hant"
        target_gl = "HK"

    encoded_keyword = urllib.parse.quote(search_query)
    target_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl={lang}&gl={target_gl}&ceid={target_ceid}"
    
    try:
        feed = feedparser.parse(target_url)
        data = []
        for entry in feed.entries:
            try: pub_date = datetime(*entry.published_parsed[:6])
            except: pub_date = datetime.now()
            data.append({
                "Select": False,
                "Date": pub_date,
                "Type": "ğŸ“° æ–°è",
                "Title": entry.title,
                "Source": entry.source.title if 'source' in entry else "Google News",
                "Link": entry.link
            })
        return pd.DataFrame(data)
    except: return pd.DataFrame()

# B. é€šç”¨å…¨ç¶²/ç‰¹å®šå¹³å°çˆ¬èŸ² (DuckDuckGo)
def fetch_web_search(keyword, region_code, time_range, platform_mode=None):
    if region_code == "US": ddg_region = "us-en"
    elif region_code == "CA": ddg_region = "ca-en"
    elif region_code == "HK": ddg_region = "hk-tzh"
    else: ddg_region = "wt-wt"
    
    ddg_time = None 
    if time_range == "éå»ä¸€å¤©": ddg_time = "d"
    elif time_range == "éå»ä¸€é€±": ddg_time = "w"
    elif time_range == "éå»ä¸€å€‹æœˆ": ddg_time = "m"
    elif time_range == "éå»ä¸€å¹´": ddg_time = "y"
    
    final_keyword = keyword
    search_region = ddg_region
    source_type = "ğŸŒ å…¨ç¶²"

    if platform_mode == "reddit":
        final_keyword = f"{keyword} site:reddit.com"
        source_type = "ğŸ’¬ Reddit"
    elif platform_mode == "pinterest":
        final_keyword = f"{keyword} site:pinterest.com"
        source_type = "ğŸ“Œ Pinterest"
    else:
        source_type = "ğŸŒ è«–å£‡/éƒ¨è½æ ¼"
        is_chinese_query = any(u'\u4e00' <= c <= u'\u9fff' for c in keyword)
        if (region_code in ["US", "CA"]) and is_chinese_query:
            search_region = "wt-wt"
            if region_code == "US": final_keyword = f"{keyword} (ç¾åœ‹ OR åŒ—ç¾ OR è¯äºº)"
            elif region_code == "CA": final_keyword = f"{keyword} (åŠ æ‹¿å¤§ OR æ¸©å“¥è¯ OR å¤šå€«å¤š)"

    try:
        results = DDGS().text(keywords=final_keyword, region=search_region, time=ddg_time, max_results=30)
        data = []
        if results:
            for r in results:
                link = r.get('href', '')
                title = r.get('title', '')
                if link and title:
                    try: source_domain = urllib.parse.urlparse(link).netloc
                    except: source_domain = "Web"
                    data.append({
                        "Select": False,
                        "Date": datetime.now(),
                        "Type": source_type,
                        "Title": title,
                        "Source": source_domain,
                        "Link": link
                    })
        return pd.DataFrame(data)
    except Exception as e:
        print(f"Error: {e}")
        return pd.DataFrame()

# C. æ··åˆæœç´¢æ§åˆ¶å™¨ (ğŸ”¥ é‡é»ä¿®å¾©å€)
def run_hybrid_search(keyword, location_choice, search_types, time_range):
    frames = []
    
    # å®šç¾©ä»»å‹™
    if location_choice == "ğŸ‡ºğŸ‡¸ ç¾åœ‹ (US)":
        news_tasks = [("en-US", "US"), ("zh-TW", "US")]
        region_code = "US"
    elif location_choice == "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ (CA)":
        news_tasks = [("en-CA", "CA"), ("zh-TW", "CA")]
        region_code = "CA"
    elif location_choice == "ğŸ‡­ğŸ‡° é¦™æ¸¯ (HK)":
        news_tasks = [("zh-HK", "HK"), ("en-HK", "HK")]
        region_code = "HK"
    
    # åŸ·è¡Œä»»å‹™
    if "æ–°èåª’é«” (News)" in search_types:
        if time_range in ["ä¸é™æ™‚é–“ (é è¨­)", "éå»ä¸€å¤©", "éå»ä¸€é€±", "éå»ä¸€å€‹æœˆ"]:
            for lang, region in news_tasks:
                frames.append(fetch_google_news(keyword, lang, region))
            
    if "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Web/Blogs)" in search_types:
        frames.append(fetch_web_search(keyword, region_code, time_range, platform_mode=None))

    if "Reddit è¨è«–å€" in search_types:
        frames.append(fetch_web_search(keyword, region_code, time_range, platform_mode="reddit"))

    if "Pinterest éˆæ„Ÿ" in search_types:
        frames.append(fetch_web_search(keyword, region_code, time_range, platform_mode="pinterest"))

    # åˆä½µçµæœ (ğŸ”¥ é€™è£¡åŠ äº†é˜²å‘†æ©Ÿåˆ¶)
    if frames:
        result = pd.concat(frames)
        
        # 1. å¦‚æœåˆä½µå‡ºä¾†æ˜¯ç©ºçš„ (å…¨è»è¦†æ²’)ï¼Œç›´æ¥å›å‚³ç©ºè¡¨ï¼Œä¸è¦å¾€ä¸‹è·‘
        if result.empty:
            return pd.DataFrame(columns=['Select', 'Type', 'Date', 'Title', 'Link', 'Source'])
            
        # 2. å¦‚æœå› ç‚ºæŸäº›åŸå›  Select æ¬„ä½ä¸è¦‹äº†ï¼ŒæŠŠå®ƒåŠ å›å»
        if 'Select' not in result.columns:
            result['Select'] = False

        result = result.drop_duplicates(subset=['Link'])
        
        # 3. å®‰å…¨åœ°æ’åºæ¬„ä½
        cols = ['Select'] + [c for c in result.columns if c != 'Select']
        return result[cols]
    else:
        # å¦‚æœæ ¹æœ¬æ²’æœ‰ä»»å‹™åŸ·è¡Œ
        return pd.DataFrame(columns=['Select', 'Type', 'Date', 'Title', 'Link', 'Source'])

# --- 4. å´é‚Šæ¬„å°èˆª ---
with st.sidebar:
    st.title("ğŸ—‚ï¸ ç³»çµ±å°èˆª")
    page = st.radio("å‰å¾€å°ˆå€", ["ğŸ” æƒ…å ±æœå°‹", "ğŸ“‚ ç«¶å“è³‡æ–™å¤¾"])
    st.divider()
    
    st.subheader("âš™ï¸ è³‡æ–™å¤¾ç®¡ç†")
    new_folder = st.text_input("æ–°å¢è³‡æ–™å¤¾", placeholder="ä¾‹å¦‚: Pinterest éˆæ„Ÿæ¿")
    if st.button("â• æ–°å¢"):
        if new_folder and new_folder not in st.session_state.folder_list:
            st.session_state.folder_list.append(new_folder)
            st.success(f"å·²æ–°å¢: {new_folder}")
            st.rerun()
    st.caption(f"å·²æ”¶è—: {len(st.session_state.favorites)} ç­†")

# --- 5. é é¢é‚è¼¯ ---

if page == "ğŸ” æƒ…å ±æœå°‹":
    st.title("ğŸ” æƒ…å ±æœå°‹")
    
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        search_kw = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚: Kitchen Island Ideas, Range Hood...")
    with col2:
        location = st.selectbox("ç›®æ¨™å¸‚å ´", ["ğŸ‡ºğŸ‡¸ ç¾åœ‹ (US)", "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ (CA)", "ğŸ‡­ğŸ‡° é¦™æ¸¯ (HK)"])
    with col3:
        time_range = st.selectbox("æ™‚é–“ç¯„åœ", ["ä¸é™æ™‚é–“ (é è¨­)", "éå»ä¸€å¤©", "éå»ä¸€é€±", "éå»ä¸€å€‹æœˆ", "éå»ä¸€å¹´"])

    search_scope = st.multiselect(
        "é¸æ“‡æœå°‹é »é“", 
        ["æ–°èåª’é«” (News)", "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Web/Blogs)", "Reddit è¨è«–å€", "Pinterest éˆæ„Ÿ"],
        default=["æ–°èåª’é«” (News)"]
    )

    if st.button("ğŸš€ é–‹å§‹æœå°‹", type="primary"):
        if search_kw:
            with st.spinner("æ­£åœ¨å„å¤§å¹³å°æƒæä¸­..."):
                st.session_state.search_results = run_hybrid_search(search_kw, location, search_scope, time_range)
