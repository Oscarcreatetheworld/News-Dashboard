import streamlit as st
import pandas as pd
import feedparser
import urllib.parse
from datetime import datetime, timedelta
import altair as alt

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="Daily Monitoring", page_icon="ğŸ³", layout="wide")
st.title("ğŸ³ æ–°èè³‡æ–™åº«")

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•¸ ---

# A. åŸºç¤çˆ¬èŸ²å‡½æ•¸
def fetch_rss(keyword, lang, region):
    encoded_keyword = urllib.parse.quote(keyword)
    # çµ„åˆ Google News RSS URL
    # é€™è£¡æœƒæ ¹æ“šå‚³å…¥çš„ lang (ä¾‹å¦‚ zh-TW æˆ– en-US) è‡ªå‹•èª¿æ•´æœç´¢æº
    ceid_lang = lang.split('-')[0]
    target_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl={lang}&gl={region}&ceid={region}:{lang}"
    
    feed = feedparser.parse(target_url)
    data = []
    for entry in feed.entries:
        try:
            pub_date = datetime(*entry.published_parsed[:6])
        except:
            pub_date = datetime.now()
            
        data.append({
            "Date": pub_date,
            "Title": entry.title,
            "Source": entry.source.title if 'source' in entry else "N/A",
            "Link": entry.link,
            "Lang": "ä¸­æ–‡" if "zh" in lang else "English" # æ¨™è¨˜ä¾†æºèªè¨€
        })
    return pd.DataFrame(data)

# B. æ•´åˆæœç´¢å‡½æ•¸ (é€™è£¡å°±æ˜¯ã€Œä¸­è‹±é€šåƒã€çš„é—œéµ)
def fetch_mixed_news(keyword, location_choice):
    # å®šç¾©æ¯å€‹åœ°å€è¦æœå“ªäº›èªè¨€
    # æ ¼å¼: (èªè¨€ä»£ç¢¼, åœ°å€ä»£ç¢¼)
    tasks = []
    
    if location_choice == "ğŸ‡ºğŸ‡¸ ç¾åœ‹ (US)":
        tasks = [
            ("en-US", "US"), # æœç¾åœ‹è‹±æ–‡
            ("zh-TW", "US")  # æœç¾åœ‹ä¸­æ–‡ (ç¹é«”)
        ]
    elif location_choice == "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ (CA)":
        tasks = [
            ("en-CA", "CA"), # æœåŠ æ‹¿å¤§è‹±æ–‡
            ("zh-TW", "CA")  # æœåŠ æ‹¿å¤§ä¸­æ–‡
        ]

    # é–‹å§‹åŸ·è¡Œé›™è»Œæœç´¢
    frames = []
    for lang, region in tasks:
        df = fetch_rss(keyword, lang, region)
        frames.append(df)
    
    # åˆä½µçµæœ
    if frames:
        result_df = pd.concat(frames)
        # å»é™¤é‡è¤‡ (å¦‚æœåŒä¸€ç¯‡æ–°èè¢«é‡è¤‡æŠ“åˆ°)
        result_df = result_df.drop_duplicates(subset=['Link'])
        # ä¾ç…§æ—¥æœŸæ’åº (æ–°çš„åœ¨ä¸Šé¢)
        result_df = result_df.sort_values(by='Date', ascending=False)
        return result_df
    else:
        return pd.DataFrame()

# C. è³‡æ–™åº«è®€å–
@st.cache_data(ttl=600)
def load_historical_data(url):
    try:
        df = pd.read_csv(url)
        df['Date'] = pd.to_datetime(df['Date'])
        return df
    except:
        return pd.DataFrame()

# --- 3. å´é‚Šæ¬„è¨­å®š ---
with st.sidebar:
    st.header("âš™ï¸ ç³»çµ±è¨­å®š")
    mode = st.radio("é¸æ“‡æ¨¡å¼", ["ğŸ“¡ å³æ™‚åµå¯Ÿ (Live)", "ğŸ—„ï¸ æ­·å²è³‡æ–™åº« (DB)"])
    st.divider()
    
    # æ—¥æœŸç¯©é¸
    today = datetime.now().date()
    start_date = st.date_input("è³‡æ–™åº«-é–‹å§‹", today - timedelta(days=180))
    end_date = st.date_input("è³‡æ–™åº«-çµæŸ", today)

# --- 4. ä¸»ç•«é¢é‚è¼¯ ---

# === æ¨¡å¼ä¸€ï¼šå³æ™‚åµå¯Ÿ (Live) ===
if mode == "ğŸ“¡ å³æ™‚åµå¯Ÿ (Live)":
    st.subheader("ğŸ“¡ å…¨ç¶²å³æ™‚æœç´¢ (ä¸­è‹±æ··åˆ)")
    st.markdown("è¼¸å…¥é—œéµå­—ï¼Œç³»çµ±å°‡è‡ªå‹•åŒæ™‚æƒæè©²åœ°å€çš„ã€Œè‹±æ–‡ä¸»æµåª’é«”ã€èˆ‡ã€Œè¯äººç¤¾ç¾¤åª’é«”ã€ã€‚")
    
    col1, col2 = st.columns([3, 1])
    with col1:
        search_kw = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚: Range Hood, æ–¹å¤ª, Fotile...")
    with col2:
        # é€™è£¡è®Šç°¡å–®äº†ï¼åªé¸åœ°é»
        location = st.selectbox("é¸æ“‡å¸‚å ´å€åŸŸ", ["ğŸ‡ºğŸ‡¸ ç¾åœ‹ (US)", "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ (CA)"])
    
    if st.button("ğŸš€ é–‹å§‹æœç´¢", type="primary"):
        if search_kw:
            with st.spinner(f"æ­£åœ¨åŒæ™‚æƒæ {location} çš„ä¸­è‹±æ–‡æƒ…å ±..."):
                live_df = fetch_mixed_news(search_kw, location)
                
                if not live_df.empty:
                    st.success(f"æœå°‹å®Œæˆï¼æ‰¾åˆ° {len(live_df)} ç­†è³‡æ–™")
                    
                    st.data_editor(
                        live_df,
                        column_config={
                            "Link": st.column_config.LinkColumn("é€£çµ", display_text="é»æ“Šé–±è®€"),
                            "Date": st.column_config.DateColumn("ç™¼å¸ƒæ™‚é–“", format="YYYY-MM-DD HH:mm"),
                            "Title": st.column_config.TextColumn("æ¨™é¡Œ"),
                            "Lang": st.column_config.TextColumn("èªç³»", width="small"),
                        },
                        use_container_width=True
                    )
                else:
                    st.warning("æ‰¾ä¸åˆ°è³‡æ–™ï¼Œè«‹æª¢æŸ¥é—œéµå­—ã€‚")
        else:
            st.error("è«‹è¼¸å…¥é—œéµå­—ï¼")

# === æ¨¡å¼äºŒï¼šæ­·å²è³‡æ–™åº« (DB) ===
elif mode == "ğŸ—„ï¸ æ­·å²è³‡æ–™åº« (DB)":
    st.subheader("ğŸ—„ï¸ å…§éƒ¨è¼¿æƒ…è³‡æ–™åº«")
    
    # ğŸ”¥ è«‹è¨˜å¾—æŠŠé€™è£¡æ›æˆä½ çš„ CSV ç¶²å€ ğŸ”¥
    sheet_url = "ä½ çš„_GOOGLE_SHEET_CSV_é€£çµ"
    
    df = load_historical_data(sheet_url)
    
    if not df.empty:
        mask = (df['Date'].dt.date >= start_date) & (df['Date'].dt.date <= end_date)
        filtered_df = df.loc[mask]
        
        db_search = st.text_input("æœå°‹æ­·å²è³‡æ–™...", placeholder="è¼¸å…¥é—œéµå­—...")
        if db_search:
            filtered_df = filtered_df[filtered_df['Title'].str.contains(db_search, case=False, na=False)]
            
        st.metric("è³‡æ–™ç­†æ•¸", len(filtered_df))
        st.data_editor(
            filtered_df[['Date', 'Category', 'Title', 'Source', 'Link']],
            column_config={
                "Link": st.column_config.LinkColumn("é€£çµ", display_text="Go"),
                "Date": st.column_config.DateColumn("æ—¥æœŸ", format="YYYY-MM-DD"),
            },
            hide_index=True,
            use_container_width=True
        )
    else:
        st.error("ç„¡æ³•è®€å–è³‡æ–™åº«ï¼Œè«‹æª¢æŸ¥ CSV ç¶²å€ã€‚")
