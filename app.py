import streamlit as st
import pandas as pd
import feedparser
import urllib.parse
from datetime import datetime, timedelta
from duckduckgo_search import DDGS

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="æµ·å¤–ç„¡æ•µ", page_icon="ğŸ“¡", layout="wide")
st.title("ğŸ“¡ æµ·å¤–ç„¡æ•µæœå°‹å¼•æ“")

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•¸ ---

# A. Google News çˆ¬èŸ² (ä¿®å¾©ç‰ˆ)
def fetch_google_news(keyword, lang, region):
    # --- é—œéµä¿®æ­£ï¼šé‡å°åŒ—ç¾ä¸­æ–‡çš„ç‰¹æ®Šè™•ç† ---
    # å¦‚æœæ˜¯åœ¨ç¾åœ‹/åŠ æ‹¿å¤§æœä¸­æ–‡ï¼Œæˆ‘å€‘ä¸è¦å¼·åˆ¶è¨­å®š gl=USï¼Œå› ç‚ºé‚£æœƒæ¿¾æ‰å¾ˆå¤šè¯äººåª’é«”
    # æ”¹ç‚ºï¼šä½¿ç”¨é—œéµå­—é™å®šï¼Œä½†æ”¾å¯¬åœ°å€é™åˆ¶
    
    search_query = keyword
    target_gl = region
    target_ceid = f"{region}:{lang.split('-')[0]}"
    
    # ç‰¹æ®Šé‚è¼¯ï¼šå¦‚æœæ˜¯åŒ—ç¾åœ°å€çš„ä¸­æ–‡æœå°‹
    if (region in ["US", "CA"]) and ("zh" in lang):
        # 1. è‡ªå‹•å¹«é—œéµå­—åŠ æ–™ (Keyword Injection)
        if region == "US":
            search_query = f"{keyword} (ç¾åœ‹ OR åŒ—ç¾ OR USA)"
        elif region == "CA":
            search_query = f"{keyword} (åŠ æ‹¿å¤§ OR Canada OR æ¸©å“¥å OR å¤šä¼¦å¤š)"
            
        # 2. æ”¾å¯¬åœ°å€è¨­å®šï¼šæ”¹ç”¨å°ç£ä»‹é¢æœï¼Œä½†é—œéµå­—å«åœ°å€
        # é€™æ¨£æœ€å®¹æ˜“æœåˆ°ä¸–ç•Œæ—¥å ±ã€æˆ–æ˜¯å°ç£äººè¨è«–ç¾åœ‹ç”Ÿæ´»çš„æ–‡ç« 
        target_gl = "TW" 
        target_ceid = "TW:zh-Hant"

    encoded_keyword = urllib.parse.quote(search_query)
    
    # é¦™æ¸¯ç¶­æŒåŸæ¨£
    if region == "HK" and "zh" in lang:
        target_ceid = "HK:zh-Hant"
        target_gl = "HK"

    target_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl={lang}&gl={target_gl}&ceid={target_ceid}"
    
    try:
        feed = feedparser.parse(target_url)
        data = []
        for entry in feed.entries:
            try:
                pub_date = datetime(*entry.published_parsed[:6])
            except:
                pub_date = datetime.now()
                
            data.append({
                "Date": pub_date,
                "Type": "æ–°è (News)",
                "Title": entry.title,
                "Source": entry.source.title if 'source' in entry else "Google News",
                "Link": entry.link,
                "Snippet": "é»æ“Šé–±è®€å…¨æ–‡..."
            })
        return pd.DataFrame(data)
    except:
        return pd.DataFrame()

# B. DuckDuckGo å…¨ç¶²çˆ¬èŸ² (ä¿®å¾©ç‰ˆ)
def fetch_web_search(keyword, region_code, time_range):
    # region_code è½‰æ›
    if region_code == "US": ddg_region = "us-en"
    elif region_code == "CA": ddg_region = "ca-en"
    elif region_code == "HK": ddg_region = "hk-tzh"
    else: ddg_region = "wt-wt"
    
    # æ™‚é–“åƒæ•¸
    ddg_time = None 
    if time_range == "éå»ä¸€å¤©": ddg_time = "d"
    elif time_range == "éå»ä¸€é€±": ddg_time = "w"
    elif time_range == "éå»ä¸€å€‹æœˆ": ddg_time = "m"
    elif time_range == "éå»ä¸€å¹´": ddg_time = "y"
    
    # --- é—œéµä¿®æ­£ï¼šDuckDuckGo çš„ä¸­æ–‡æœå°‹å„ªåŒ– ---
    # å¦‚æœæ˜¯åœ¨ç¾åœ‹æœä¸­æ–‡ï¼Œæˆ‘å€‘ä¸è¦é–å®š region="us-en" (é‚£æœƒåªæœè‹±æ–‡ç¶²ç«™)
    # æˆ‘å€‘æ”¹ç”¨å…¨çƒæœç´¢ (wt-wt)ï¼Œä½†æ˜¯åŠ ä¸Šåœ°å€é—œéµå­—
    
    final_keyword = keyword
    search_region = ddg_region
    
    # åˆ¤æ–·æ˜¯å¦åŒ…å«ä¸­æ–‡å­— (ç°¡å–®åˆ¤æ–·)
    is_chinese_query = any(u'\u4e00' <= c <= u'\u9fff' for c in keyword)
    
    if (region_code in ["US", "CA"]) and is_chinese_query:
        search_region = "wt-wt" # æ”¾å¯¬åˆ°å…¨çƒ
        if region_code == "US":
            final_keyword = f"{keyword} (ç¾åœ‹ OR åŒ—ç¾ OR è¯äºº)"
        elif region_code == "CA":
            final_keyword = f"{keyword} (åŠ æ‹¿å¤§ OR æ¸©å“¥è¯ OR å¤šå€«å¤š)"

    try:
        results = DDGS().text(keywords=final_keyword, region=search_region, time=ddg_time, max_results=40)
        
        data = []
        for r in results:
            data.append({
                "Date": datetime.now(),
                "Type": "å…¨ç¶² (Web/Forum)",
                "Title": r['title'],
                "Source": urllib.parse.urlparse(r['href']).netloc,
                "Link": r['href'],
                "Snippet": r['body']
            })
        return pd.DataFrame(data)
    except Exception as e:
        return pd.DataFrame()

# C. æ··åˆæœç´¢æ§åˆ¶å™¨
def run_hybrid_search(keyword, location_choice, search_types, time_range):
    frames = []
    
    # å®šç¾©ä»»å‹™
    if location_choice == "ğŸ‡ºğŸ‡¸ ç¾åœ‹ (US)":
        news_tasks = [("en-US", "US"), ("zh-TW", "US")]
        region_code = "US"
    elif location_choice == "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ (CA)":
        news_tasks = [("en-CA", "CA"), ("zh-TW", "CA")]
        region_code = "CA"
    elif location_choice == "ğŸ‡­ğŸ‡° é¦™æ¸¯ (HK)":
        news_tasks = [("zh-HK", "HK"), ("en-HK", "HK")]
        region_code = "HK"
    
    # 1. è·‘æ–°è
    if "æ–°èåª’é«” (News)" in search_types:
        if time_range in ["ä¸é™æ™‚é–“ (é è¨­)", "éå»ä¸€å¤©", "éå»ä¸€é€±", "éå»ä¸€å€‹æœˆ"]:
            for lang, region in news_tasks:
                df = fetch_google_news(keyword, lang, region)
                frames.append(df)
            
    # 2. è·‘å…¨ç¶²
    if "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Forums/Blogs)" in search_types:
        df_web = fetch_web_search(keyword, region_code, time_range)
        frames.append(df_web)

    if frames:
        result_df = pd.concat(frames)
        result_df = result_df.drop_duplicates(subset=['Link'])
        return result_df
    else:
        return pd.DataFrame()

# D. è³‡æ–™åº«è®€å–
@st.cache_data(ttl=600)
def load_historical_data(url):
    try:
        df = pd.read_csv(url)
        df['Date'] = pd.to_datetime(df['Date'])
        return df
    except:
        return pd.DataFrame()

# --- 3. å´é‚Šæ¬„ ---
with st.sidebar:
    st.header("âš™ï¸ é›·é”è¨­å®š")
    mode = st.radio("æ¨¡å¼", ["ğŸ“¡ å…¨ç¶²æƒæ (Live)", "ğŸ—„ï¸ æ­·å²è³‡æ–™åº«"])
    st.divider()
    today = datetime.now().date()
    start_date = st.date_input("è³‡æ–™åº«èµ·å§‹æ—¥", today - timedelta(days=180))

# --- 4. ä¸»ç•«é¢ ---

if mode == "ğŸ“¡ å…¨ç¶²æƒæ (Live)":
    st.subheader("ğŸ“¡ å…¨çƒå»šé›»å…¨ç¶²æƒæ (Pro)")
    
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        search_kw = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚: æŠ½æ²¹ç…™æ©Ÿ, æ–¹å¤ª, Robam...")
    with col2:
        location = st.selectbox("ç›®æ¨™å¸‚å ´", ["ğŸ‡ºğŸ‡¸ ç¾åœ‹ (US)", "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ (CA)", "ğŸ‡­ğŸ‡° é¦™æ¸¯ (HK)"])
    with col3:
        time_range = st.selectbox("â±ï¸ æ™‚é–“ç¯„åœ", ["ä¸é™æ™‚é–“ (é è¨­)", "éå»ä¸€å¤©", "éå»ä¸€é€±", "éå»ä¸€å€‹æœˆ", "éå»ä¸€å¹´"])
        
    search_scope = st.multiselect(
        "é¸æ“‡æœå°‹ä¾†æº",
        ["æ–°èåª’é«” (News)", "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Forums/Blogs)"],
        default=["æ–°èåª’é«” (News)", "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Forums/Blogs)"]
    )
    
    if st.button("ğŸš€ æœå°‹", type="primary"):
        if search_kw:
            with st.spinner(f"æ­£åœ¨æŒ–æ˜ {location} çš„ä¸­è‹±æ–‡æƒ…å ± (å·²å•Ÿç”¨æ™ºæ…§é—œéµå­—å„ªåŒ–)..."):
                df = run_hybrid_search(search_kw, location, search_scope, time_range)
                
                if not df.empty:
                    st.success(f"æƒæå®Œæˆï¼å…±ç™¼ç¾ {len(df)} ç­†æƒ…å ±")
                    st.data_editor(
                        df,
                        column_config={
                            "Link": st.column_config.LinkColumn("é€£çµ", display_text="Go"),
                            "Date": st.column_config.DateColumn("ç™¼å¸ƒ/æŠ“å–æ—¥", format="YYYY-MM-DD"),
                            "Title": st.column_config.TextColumn("æ¨™é¡Œ", width="medium"),
                            "Snippet": st.column_config.TextColumn("æ‘˜è¦", width="large"),
                        },
                        use_container_width=True
                    )
                else:
                    st.warning("æ‰¾ä¸åˆ°çµæœã€‚")
        else:
            st.error("è«‹è¼¸å…¥é—œéµå­—")

elif mode == "ğŸ—„ï¸ æ­·å²è³‡æ–™åº«":
    st.subheader("ğŸ—„ï¸ å…§éƒ¨è¼¿æƒ…è³‡æ–™åº«")
    sheet_url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vQai1zkVJlpDcZhzs76S_JiCsm1JogWxdYlw4vA4k1IeWLHqiReRRY29xQm7ephIk9QJfri7OlvfdmF/pubhtml"
    
    df = load_historical_data(sheet_url)
    if not df.empty:
        mask = (df['Date'].dt.date >= start_date)
        filtered_df = df.loc[mask]
        
        db_search = st.text_input("æœå°‹è³‡æ–™åº«...", placeholder="è¼¸å…¥é—œéµå­—...")
        if db_search:
            filtered_df = filtered_df[filtered_df['Title'].str.contains(db_search, case=False, na=False)]
            
        st.data_editor(filtered_df, use_container_width=True, hide_index=True)
    else:
        st.error("ç„¡æ³•è®€å–è³‡æ–™åº«")
