import streamlit as st
import pandas as pd
import feedparser
import urllib.parse
from datetime import datetime, timedelta
from duckduckgo_search import DDGS

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="æµ·å¤–æ–°èåœ–æ›¸é¤¨", page_icon="ğŸ“¡", layout="wide")
st.title("ğŸ“¡ å…¨æµ·å¤–æ–°èåœ–æ›¸é¤¨ (NA + HK)")

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•¸ ---

# A. Google News çˆ¬èŸ²
def fetch_google_news(keyword, lang, region):
    encoded_keyword = urllib.parse.quote(keyword)
    # Google RSS URL çµ„åˆé‚è¼¯
    # é¦™æ¸¯ç‰¹åˆ¥è™•ç†: ceid=HK:zh-Hant (ä¸­æ–‡), ceid=HK:en (è‹±æ–‡)
    if region == "HK" and "zh" in lang:
        target_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl={lang}&gl={region}&ceid={region}:zh-Hant"
    else:
        ceid_lang = lang.split('-')[0]
        target_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl={lang}&gl={region}&ceid={region}:{ceid_lang}"
    
    try:
        feed = feedparser.parse(target_url)
        data = []
        for entry in feed.entries:
            try:
                pub_date = datetime(*entry.published_parsed[:6])
            except:
                pub_date = datetime.now()
                
            data.append({
                "Date": pub_date,
                "Type": "æ–°è (News)",
                "Title": entry.title,
                "Source": entry.source.title if 'source' in entry else "Google News",
                "Link": entry.link,
                "Snippet": "é»æ“Šé€£çµé–±è®€å…¨æ–‡..."
            })
        return pd.DataFrame(data)
    except Exception as e:
        return pd.DataFrame()

# B. DuckDuckGo å…¨ç¶²çˆ¬èŸ²
def fetch_web_search(keyword, region_code):
    # è¨­å®š DDG çš„åœ°å€ä»£ç¢¼
    if region_code == "US": ddg_region = "us-en"
    elif region_code == "CA": ddg_region = "ca-en"
    elif region_code == "HK": ddg_region = "hk-tzh" # é¦™æ¸¯ç¹é«”
    else: ddg_region = "wt-wt"
    
    try:
        # max_results è¨­å®šæŠ“ 25 ç­†ï¼Œé¿å…å¤ªæ…¢
        results = DDGS().text(keywords=keyword, region=ddg_region, max_results=25)
        
        data = []
        for r in results:
            data.append({
                "Date": datetime.now(),
                "Type": "å…¨ç¶² (Forum/Blog)",
                "Title": r['title'],
                "Source": urllib.parse.urlparse(r['href']).netloc,
                "Link": r['href'],
                "Snippet": r['body']
            })
        return pd.DataFrame(data)
    except Exception as e:
        # st.error(f"å…¨ç¶²æœç´¢éŒ¯èª¤: {e}") # æš«æ™‚éš±è—éŒ¯èª¤è¨Šæ¯è®“ä»‹é¢ä¹¾æ·¨
        return pd.DataFrame()

# C. æ··åˆæœç´¢æ§åˆ¶å™¨ (æ–°å¢é¦™æ¸¯é‚è¼¯)
def run_hybrid_search(keyword, location_choice, search_types):
    frames = []
    
    # å®šç¾©åœ°å€ä»»å‹™æ¸…å–®
    if location_choice == "ğŸ‡ºğŸ‡¸ ç¾åœ‹ (US)":
        news_tasks = [("en-US", "US"), ("zh-TW", "US")]
        ddg_region = "US"
    elif location_choice == "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ (CA)":
        news_tasks = [("en-CA", "CA"), ("zh-TW", "CA")]
        ddg_region = "CA"
    elif location_choice == "ğŸ‡­ğŸ‡° é¦™æ¸¯ (HK)":
        # é¦™æ¸¯ï¼šåŒæ™‚æœä¸­æ–‡(zh-HK)èˆ‡è‹±æ–‡(en-HK)
        news_tasks = [("zh-HK", "HK"), ("en-HK", "HK")]
        ddg_region = "HK"
    
    # 1. è·‘æ–°è
    if "æ–°èåª’é«” (News)" in search_types:
        for lang, region in news_tasks:
            df = fetch_google_news(keyword, lang, region)
            frames.append(df)
            
    # 2. è·‘è«–å£‡
    if "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Forums/Blogs)" in search_types:
        # é‡å°é¦™æ¸¯è«–å£‡å„ªåŒ–ï¼šå¯ä»¥åœ¨é€™è£¡å¹«é—œéµå­—åŠ æ–™
        # ä¾‹å¦‚: if ddg_region == "HK": keyword += " site:.hk" (é€™æ˜¯ä¸€å€‹é€²éšæŠ€å·§ï¼Œç›®å‰å…ˆä¸åŠ )
        df_web = fetch_web_search(keyword, ddg_region)
        frames.append(df_web)

    if frames:
        result_df = pd.concat(frames)
        result_df = result_df.drop_duplicates(subset=['Link'])
        return result_df
    else:
        return pd.DataFrame()

# D. è³‡æ–™åº«è®€å–
@st.cache_data(ttl=600)
def load_historical_data(url):
    try:
        df = pd.read_csv(url)
        df['Date'] = pd.to_datetime(df['Date'])
        return df
    except:
        return pd.DataFrame()

# --- 3. å´é‚Šæ¬„ ---
with st.sidebar:
    st.header("âš™ï¸ é›·é”è¨­å®š")
    mode = st.radio("æ¨¡å¼", ["ğŸ“¡ å…¨ç¶²æƒæ (Live)", "ğŸ—„ï¸ æ­·å²è³‡æ–™åº«"])
    st.divider()
    today = datetime.now().date()
    start_date = st.date_input("è³‡æ–™åº«èµ·å§‹æ—¥", today - timedelta(days=180))

# --- 4. ä¸»ç•«é¢ ---

if mode == "ğŸ“¡ å…¨ç¶²æƒæ (Live)":
    st.subheader("ğŸ“¡ å…¨çƒå»šé›»å…¨ç¶²æƒæ")
    st.markdown("æ”¯æ´åœ°å€ï¼šğŸ‡ºğŸ‡¸ ç¾åœ‹ã€ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ã€ğŸ‡­ğŸ‡° é¦™æ¸¯")
    
    col1, col2 = st.columns([3, 1])
    with col1:
        search_kw = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚: æŠ½æ²¹ç…™æ©Ÿ, æ´—ç¢—æ©Ÿ, Miele, German Pool...")
    with col2:
        # æ–°å¢é¦™æ¸¯é¸é …
        location = st.selectbox("ç›®æ¨™å¸‚å ´", ["ğŸ‡ºğŸ‡¸ ç¾åœ‹ (US)", "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ (CA)", "ğŸ‡­ğŸ‡° é¦™æ¸¯ (HK)"])
        
    search_scope = st.multiselect(
        "é¸æ“‡æœå°‹ä¾†æº",
        ["æ–°èåª’é«” (News)", "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Forums/Blogs)"],
        default=["æ–°èåª’é«” (News)", "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Forums/Blogs)"]
    )
    
    if st.button("ğŸš€ ç™¼å°„é›·é”", type="primary"):
        if search_kw:
            with st.spinner(f"æ­£åœ¨æƒæ {location} çš„ç›¸é—œæƒ…å ±..."):
                df = run_hybrid_search(search_kw, location, search_scope)
                
                if not df.empty:
                    st.success(f"æƒæå®Œæˆï¼å…±ç™¼ç¾ {len(df)} ç­†æƒ…å ±")
                    st.data_editor(
                        df,
                        column_config={
                            "Link": st.column_config.LinkColumn("é€£çµ", display_text="Go"),
                            "Date": st.column_config.DateColumn("æ—¥æœŸ", format="YYYY-MM-DD"),
                            "Title": st.column_config.TextColumn("æ¨™é¡Œ", width="medium"),
                            "Type": st.column_config.TextColumn("ä¾†æº", width="small"),
                            "Snippet": st.column_config.TextColumn("æ‘˜è¦", width="large"),
                        },
                        use_container_width=True
                    )
                else:
                    st.warning("æœªæœå°‹åˆ°çµæœã€‚å»ºè­°ï¼š\n1. é¦™æ¸¯æœå°‹å»ºè­°ç”¨ç¹é«”ä¸­æ–‡\n2. è©¦è‘—æœå°‹ç•¶åœ°å“ç‰Œ (å¦‚: German Pool, å¾·åœ‹å¯¶)")
        else:
            st.error("è«‹è¼¸å…¥é—œéµå­—")

elif mode == "ğŸ—„ï¸ æ­·å²è³‡æ–™åº«":
    st.subheader("ğŸ—„ï¸ å…§éƒ¨è¼¿æƒ…è³‡æ–™åº«")
    # ğŸ”¥ è¨˜å¾—æ›ä½ çš„ CSV é€£çµ ğŸ”¥
    sheet_url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vQai1zkVJlpDcZhzs76S_JiCsm1JogWxdYlw4vA4k1IeWLHqiReRRY29xQm7ephIk9QJfri7OlvfdmF/pubhtml"
    
    df = load_historical_data(sheet_url)
    if not df.empty:
        mask = (df['Date'].dt.date >= start_date)
        filtered_df = df.loc[mask]
        
        db_search = st.text_input("æœå°‹è³‡æ–™åº«...", placeholder="è¼¸å…¥é—œéµå­—...")
        if db_search:
            filtered_df = filtered_df[filtered_df['Title'].str.contains(db_search, case=False, na=False)]
            
        st.data_editor(filtered_df, use_container_width=True, hide_index=True)
    else:
        st.error("ç„¡æ³•è®€å–è³‡æ–™åº«ï¼Œè«‹æª¢æŸ¥ CSV ç¶²å€")
