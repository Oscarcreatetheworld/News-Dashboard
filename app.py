import streamlit as st
import pandas as pd
import feedparser
import urllib.parse
from datetime import datetime, timedelta
import altair as alt

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="åŒ—ç¾å»šé›»æƒ…è³‡ä¸­å¿ƒ", page_icon="ğŸ³", layout="wide")
st.title("ğŸ³ åŒ—ç¾å»šé›»æƒ…è³‡ä¸­å¿ƒ (Live & Database)")

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•¸ ---

# A. çˆ¬èŸ²å‡½æ•¸ (æ”¯æ´å¤šèªç³»ç²¾æº–æœç´¢)
def fetch_live_news(keyword, lang_code, region):
    encoded_keyword = urllib.parse.quote(keyword)
    
    # é‡å°ä¸­æ–‡æœå°‹å„ªåŒ–ï¼šå¦‚æœæ˜¯ä¸­æ–‡æ¨¡å¼ï¼Œå¼·åˆ¶è¨­å®š ceid ç‚ºåœ°å€:èªè¨€
    if "zh" in lang_code:
        # ä¾‹å¦‚æœå°‹åŒ—ç¾è¯äººå…§å®¹ï¼šceid=US:zh-Hant (ç¹é«”ä¸­æ–‡åœ¨ç¾åœ‹)
        target_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl={lang_code}&gl={region}&ceid={region}:{lang_code}"
    else:
        # è‹±æ–‡æ¨¡å¼
        target_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl={lang_code}&gl={region}&ceid={region}:{lang_code.split('-')[0]}"
    
    feed = feedparser.parse(target_url)
    data = []
    for entry in feed.entries:
        try:
            pub_date = datetime(*entry.published_parsed[:6])
        except:
            pub_date = datetime.now()
            
        data.append({
            "Date": pub_date,
            "Title": entry.title,
            "Source": entry.source.title if 'source' in entry else "N/A",
            "Link": entry.link
        })
    return pd.DataFrame(data)

# B. è³‡æ–™åº«è®€å–
@st.cache_data(ttl=600)
def load_historical_data(url):
    try:
        df = pd.read_csv(url)
        df['Date'] = pd.to_datetime(df['Date'])
        return df
    except:
        return pd.DataFrame()

# --- 3. å´é‚Šæ¬„ï¼šæ¨¡å¼åˆ‡æ› ---
with st.sidebar:
    st.header("âš™ï¸ ç³»çµ±è¨­å®š")
    mode = st.radio("é¸æ“‡æ¨¡å¼", ["ğŸ“¡ å³æ™‚åµå¯Ÿ (Live Search)", "ğŸ—„ï¸ æ­·å²è³‡æ–™åº« (Database)"])
    st.divider()
    
    # æ—¥æœŸç¯©é¸ (çµ¦è³‡æ–™åº«ç”¨çš„)
    today = datetime.now().date()
    start_date = st.date_input("è³‡æ–™åº«-é–‹å§‹æ—¥æœŸ", today - timedelta(days=180))
    end_date = st.date_input("è³‡æ–™åº«-çµæŸæ—¥æœŸ", today)

# --- 4. ä¸»ç•«é¢é‚è¼¯ ---

# === æ¨¡å¼ä¸€ï¼šå³æ™‚åµå¯Ÿ (Live Search) ===
if mode == "ğŸ“¡ å³æ™‚åµå¯Ÿ (Live Search)":
    st.subheader("ğŸ“¡ å³æ™‚å…¨ç¶²æœç´¢")
    st.markdown("è¼¸å…¥é—œéµå­—ï¼Œç«‹å³æŠ“å– Google News æœ€æ–°è³‡æ–™ã€‚")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # é€™è£¡æç¤ºä½¿ç”¨è€…å¯ä»¥è¼¸å…¥ä¸­æ–‡
        search_kw = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚: Range Hood, æŠ½æ²¹ç…™æ©Ÿ, æ–¹å¤ª, Fotile...")
    
    with col2:
        # é€™è£¡æ˜¯æœ€é‡è¦çš„ã€Œèªç³»åˆ‡æ›ã€
        target_market = st.selectbox(
            "ç›®æ¨™å¸‚å ´/èªè¨€", 
            [
                "ğŸ‡ºğŸ‡¸ ç¾åœ‹ - è‹±æ–‡åª’é«” (Mainstream)", 
                "ğŸ‡ºğŸ‡¸ ç¾åœ‹ - è¯äººåª’é«” (Chinese Community)", 
                "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ - è‹±æ–‡åª’é«”",
                "ğŸ‡¹ğŸ‡¼ å°ç£ - ç¹é«”ä¸­æ–‡ (æ¸¬è©¦ç”¨)"
            ]
        )
    
    # è¨­å®šå°æ‡‰çš„åƒæ•¸ (èªè¨€ä»£ç¢¼, åœ°å€ä»£ç¢¼)
    market_map = {
        "ğŸ‡ºğŸ‡¸ ç¾åœ‹ - è‹±æ–‡åª’é«” (Mainstream)": ("en-US", "US"),
        "ğŸ‡ºğŸ‡¸ ç¾åœ‹ - è¯äººåª’é«” (Chinese Community)": ("zh-TW", "US"), # é—œéµï¼šåœ¨ç¾åœ‹æœä¸­æ–‡
        "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ - è‹±æ–‡åª’é«”": ("en-CA", "CA"),
        "ğŸ‡¹ğŸ‡¼ å°ç£ - ç¹é«”ä¸­æ–‡ (æ¸¬è©¦ç”¨)": ("zh-TW", "TW")
    }
    
    if st.button("ğŸš€ æœå°‹", type="primary"):
        if search_kw:
            with st.spinner(f"æ­£åœ¨æœå°‹ '{search_kw}' çš„æœ€æ–°æƒ…å ±..."):
                lang, region = market_map[target_market]
                live_df = fetch_live_news(search_kw, lang, region)
                
                if not live_df.empty:
                    st.success(f"æœå°‹å®Œæˆï¼åœ¨ã€{target_market}ã€‘æ‰¾åˆ° {len(live_df)} ç­†è³‡æ–™")
                    st.data_editor(
                        live_df,
                        column_config={
                            "Link": st.column_config.LinkColumn("é€£çµ", display_text="é»æ“Šé–±è®€"),
                            "Date": st.column_config.DateColumn("ç™¼å¸ƒæ™‚é–“", format="YYYY-MM-DD HH:mm"),
                            "Title": st.column_config.TextColumn("æ¨™é¡Œ"),
                        },
                        use_container_width=True
                    )
                else:
                    st.warning(f"æ‰¾ä¸åˆ°é—œæ–¼ '{search_kw}' çš„è³‡æ–™ã€‚å»ºè­°ï¼š\n1. æª¢æŸ¥é—œéµå­—æ‹¼å¯«\n2. å¦‚æœæœä¸­æ–‡ï¼Œè«‹ç¢ºèªå³é‚Šå·²é¸æ“‡ã€Œè¯äººåª’é«”ã€")
        else:
            st.error("è«‹è¼¸å…¥é—œéµå­—ï¼")

# === æ¨¡å¼äºŒï¼šæ­·å²è³‡æ–™åº« (Database) ===
elif mode == "ğŸ—„ï¸ æ­·å²è³‡æ–™åº« (Database)":
    st.subheader("ğŸ—„ï¸ å…§éƒ¨è¼¿æƒ…è³‡æ–™åº«")
    
    # ğŸ”¥ è«‹è¨˜å¾—æŠŠé€™è£¡æ›æˆä½ çš„ CSV ç¶²å€ ğŸ”¥
    sheet_url = "ä½ çš„_GOOGLE_SHEET_CSV_é€£çµ"
    
    df = load_historical_data(sheet_url)
    
    if not df.empty:
        # æ—¥æœŸç¯©é¸
        mask = (df['Date'].dt.date >= start_date) & (df['Date'].dt.date <= end_date)
        filtered_df = df.loc[mask]
        
        # é—œéµå­—ç¯©é¸ (æ”¯æ´ä¸­æ–‡)
        db_search = st.text_input("åœ¨æ­·å²è³‡æ–™ä¸­æœå°‹...", placeholder="è¼¸å…¥é—œéµå­— (æ”¯æ´ä¸­è‹±æ–‡)...")
        if db_search:
            # case=False è®“è‹±æ–‡ä¸åˆ†å¤§å°å¯«ï¼Œä¸­æ–‡æ²’å·®
            filtered_df = filtered_df[filtered_df['Title'].str.contains(db_search, case=False, na=False)]
            
        st.metric("è³‡æ–™ç­†æ•¸", len(filtered_df))
        
        st.data_editor(
            filtered_df[['Date', 'Category', 'Title', 'Source', 'Link']],
            column_config={
                "Link": st.column_config.LinkColumn("é€£çµ", display_text="Go"),
                "Date": st.column_config.DateColumn("æ—¥æœŸ", format="YYYY-MM-DD"),
            },
            hide_index=True,
            use_container_width=True
        )
    else:
        st.error("ç„¡æ³•è®€å–è³‡æ–™åº«ï¼Œè«‹æª¢æŸ¥ CSV ç¶²å€ã€‚")
