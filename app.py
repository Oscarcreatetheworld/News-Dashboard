import streamlit as st
import pandas as pd
import feedparser
import urllib.parse
from datetime import datetime, timedelta
from duckduckgo_search import DDGS

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="å…¨çƒå»šé›»å…¨ç¶²é›·é” Pro", page_icon="ğŸ“¡", layout="wide")
st.title("ğŸ“¡ å…¨çƒå»šé›»å…¨ç¶²é›·é” Pro (å«æ™‚å…‰æ©Ÿ)")

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•¸ ---

# A. Google News çˆ¬èŸ² (åƒ…é™è¿‘æœŸ)
def fetch_google_news(keyword, lang, region):
    encoded_keyword = urllib.parse.quote(keyword)
    if region == "HK" and "zh" in lang:
        target_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl={lang}&gl={region}&ceid={region}:zh-Hant"
    else:
        ceid_lang = lang.split('-')[0]
        target_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl={lang}&gl={region}&ceid={region}:{ceid_lang}"
    
    try:
        feed = feedparser.parse(target_url)
        data = []
        for entry in feed.entries:
            try:
                pub_date = datetime(*entry.published_parsed[:6])
            except:
                pub_date = datetime.now()
                
            data.append({
                "Date": pub_date,
                "Type": "æ–°è (News)",
                "Title": entry.title,
                "Source": entry.source.title if 'source' in entry else "Google News",
                "Link": entry.link,
            })
        return pd.DataFrame(data)
    except:
        return pd.DataFrame()

# B. DuckDuckGo å…¨ç¶²çˆ¬èŸ² (æ”¯æ´æ™‚é–“å›æº¯)
def fetch_web_search(keyword, region_code, time_range):
    # region_code è½‰æ›
    if region_code == "US": ddg_region = "us-en"
    elif region_code == "CA": ddg_region = "ca-en"
    elif region_code == "HK": ddg_region = "hk-tzh"
    else: ddg_region = "wt-wt"
    
    # æ™‚é–“åƒæ•¸è½‰æ› (d=day, w=week, m=month, y=year)
    # é è¨­ä¸é™æ™‚é–“
    ddg_time = None 
    if time_range == "éå»ä¸€å¤©": ddg_time = "d"
    elif time_range == "éå»ä¸€é€±": ddg_time = "w"
    elif time_range == "éå»ä¸€å€‹æœˆ": ddg_time = "m"
    elif time_range == "éå»ä¸€å¹´": ddg_time = "y" # é€™æ˜¯ä½ è¦çš„ï¼
    
    try:
        # é€™è£¡çš„ max_results è¨­å®šå¤šä¸€é» (50ç­†)ï¼Œå› ç‚ºæˆ‘å€‘è¦æŒ–èˆŠè³‡æ–™
        results = DDGS().text(keywords=keyword, region=ddg_region, time=ddg_time, max_results=50)
        
        data = []
        for r in results:
            data.append({
                "Date": datetime.now(), # DDG ä¸ä¸€å®šå›å‚³ç²¾ç¢ºæ—¥æœŸï¼Œæ¨™è¨˜ç‚ºæœå°‹æ—¥
                "Type": "å…¨ç¶² (Web/Forum)",
                "Title": r['title'],
                "Source": urllib.parse.urlparse(r['href']).netloc,
                "Link": r['href'],
                "Snippet": r['body']
            })
        return pd.DataFrame(data)
    except Exception as e:
        return pd.DataFrame()

# C. æ··åˆæœç´¢æ§åˆ¶å™¨
def run_hybrid_search(keyword, location_choice, search_types, time_range):
    frames = []
    
    if location_choice == "ğŸ‡ºğŸ‡¸ ç¾åœ‹ (US)":
        news_tasks = [("en-US", "US"), ("zh-TW", "US")]
        ddg_region = "US"
    elif location_choice == "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ (CA)":
        news_tasks = [("en-CA", "CA"), ("zh-TW", "CA")]
        ddg_region = "CA"
    elif location_choice == "ğŸ‡­ğŸ‡° é¦™æ¸¯ (HK)":
        news_tasks = [("zh-HK", "HK"), ("en-HK", "HK")]
        ddg_region = "HK"
    
    # 1. æ–°è (Google News RSS ä¸æ”¯æ´é•·æ™‚æ®µå›æº¯ï¼Œåƒ…è·‘æœ€æ–°)
    if "æ–°èåª’é«” (News)" in search_types:
        # åªæœ‰åœ¨é¸ã€Œä¸é™ã€æˆ–ã€Œéå»ä¸€é€±/æœˆã€æ™‚æ‰è·‘ RSSï¼Œä¸ç„¶ RSS æŠ“ä¸åˆ°èˆŠçš„ä¹Ÿæ²’ç”¨
        if time_range in ["ä¸é™æ™‚é–“ (é è¨­)", "éå»ä¸€å¤©", "éå»ä¸€é€±", "éå»ä¸€å€‹æœˆ"]:
            for lang, region in news_tasks:
                df = fetch_google_news(keyword, lang, region)
                frames.append(df)
            
    # 2. å…¨ç¶² (DuckDuckGo æ”¯æ´æ™‚é–“å›æº¯)
    if "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Forums/Blogs)" in search_types:
        df_web = fetch_web_search(keyword, ddg_region, time_range)
        frames.append(df_web)

    if frames:
        result_df = pd.concat(frames)
        result_df = result_df.drop_duplicates(subset=['Link'])
        return result_df
    else:
        return pd.DataFrame()

# D. è³‡æ–™åº«è®€å–
@st.cache_data(ttl=600)
def load_historical_data(url):
    try:
        df = pd.read_csv(url)
        df['Date'] = pd.to_datetime(df['Date'])
        return df
    except:
        return pd.DataFrame()

# --- 3. å´é‚Šæ¬„ ---
with st.sidebar:
    st.header("âš™ï¸ é›·é”è¨­å®š")
    mode = st.radio("æ¨¡å¼", ["ğŸ“¡ å…¨ç¶²æƒæ (Live)", "ğŸ—„ï¸ æ­·å²è³‡æ–™åº«"])
    st.divider()
    today = datetime.now().date()
    start_date = st.date_input("è³‡æ–™åº«èµ·å§‹æ—¥", today - timedelta(days=180))

# --- 4. ä¸»ç•«é¢ ---

if mode == "ğŸ“¡ å…¨ç¶²æƒæ (Live)":
    st.subheader("ğŸ“¡ å…¨çƒå»šé›»å…¨ç¶²æƒæ (å«æ­·å²å›æº¯)")
    
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        search_kw = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚: æŠ½æ²¹ç…™æ©Ÿ, æ–¹å¤ª, Robam...")
    with col2:
        location = st.selectbox("ç›®æ¨™å¸‚å ´", ["ğŸ‡ºğŸ‡¸ ç¾åœ‹ (US)", "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§ (CA)", "ğŸ‡­ğŸ‡° é¦™æ¸¯ (HK)"])
    with col3:
        # ğŸ”¥ æ–°åŠŸèƒ½ï¼šæ™‚é–“æ™‚å…‰æ©Ÿ
        time_range = st.selectbox(
            "â±ï¸ æ™‚é–“ç¯„åœ", 
            ["ä¸é™æ™‚é–“ (é è¨­)", "éå»ä¸€å¤©", "éå»ä¸€é€±", "éå»ä¸€å€‹æœˆ", "éå»ä¸€å¹´"]
        )
        
    search_scope = st.multiselect(
        "é¸æ“‡æœå°‹ä¾†æº",
        ["æ–°èåª’é«” (News)", "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Forums/Blogs)"],
        default=["æ–°èåª’é«” (News)", "è«–å£‡èˆ‡éƒ¨è½æ ¼ (Forums/Blogs)"]
    )
    
    st.info("ğŸ’¡ æç¤ºï¼šè‹¥æƒ³æ‰¾ã€ŒåŠå¹´å‰ã€çš„èˆŠèï¼Œè«‹å°‡æ™‚é–“ç¯„åœè¨­ç‚ºã€Œéå»ä¸€å¹´ã€ï¼Œç³»çµ±æœƒæ·±å…¥æŒ–æ˜è«–å£‡èˆ‡åº«å­˜é é¢ã€‚")

    if st.button("ğŸš€ ç™¼å°„é›·é”", type="primary"):
        if search_kw:
            with st.spinner(f"æ­£åœ¨æŒ–æ˜ {time_range} é—œæ–¼ '{search_kw}' çš„æƒ…å ±..."):
                df = run_hybrid_search(search_kw, location, search_scope, time_range)
                
                if not df.empty:
                    st.success(f"æƒæå®Œæˆï¼å…±ç™¼ç¾ {len(df)} ç­†æƒ…å ±")
                    st.data_editor(
                        df,
                        column_config={
                            "Link": st.column_config.LinkColumn("é€£çµ", display_text="Go"),
                            "Date": st.column_config.DateColumn("ç™¼å¸ƒ/æŠ“å–æ—¥", format="YYYY-MM-DD"),
                            "Title": st.column_config.TextColumn("æ¨™é¡Œ", width="medium"),
                            "Snippet": st.column_config.TextColumn("æ‘˜è¦", width="large"),
                        },
                        use_container_width=True
                    )
                else:
                    st.warning("æœªæœå°‹åˆ°çµæœã€‚")
        else:
            st.error("è«‹è¼¸å…¥é—œéµå­—")

elif mode == "ğŸ—„ï¸ æ­·å²è³‡æ–™åº«":
    st.subheader("ğŸ—„ï¸ å…§éƒ¨è¼¿æƒ…è³‡æ–™åº«")
    # ğŸ”¥ è¨˜å¾—æ›ä½ çš„ CSV é€£çµ ğŸ”¥
    sheet_url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vQai1zkVJlpDcZhzs76S_JiCsm1JogWxdYlw4vA4k1IeWLHqiReRRY29xQm7ephIk9QJfri7OlvfdmF/pubhtml"
    
    df = load_historical_data(sheet_url)
    if not df.empty:
        mask = (df['Date'].dt.date >= start_date)
        filtered_df = df.loc[mask]
        
        db_search = st.text_input("æœå°‹è³‡æ–™åº«...", placeholder="è¼¸å…¥é—œéµå­—...")
        if db_search:
            filtered_df = filtered_df[filtered_df['Title'].str.contains(db_search, case=False, na=False)]
            
        st.data_editor(filtered_df, use_container_width=True, hide_index=True)
    else:
        st.error("ç„¡æ³•è®€å–è³‡æ–™åº«")
